{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"napari-lattice","text":"<p>This napari plugin allows deskewing, cropping, visualisation and designing custom analysis pipelines for lattice lightsheet data, particularly from the Zeiss Lattice Lightsheet. The plugin has also been otpimixed to run in headless mode.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Check the Wiki page for documentation on how to get started.</p> <p> </p> <p>Functions</p> <ul> <li>Deskewing and deconvolution of Zeiss lattice lightsheet images</li> <li>Ability to preview deskewed image at channel or timepoint of interest</li> <li>Crop and process only a small portion of the image </li> <li>Import ImageJ ROIs for cropping</li> <li>Create image processing workflows using napari-workflows</li> <li>Run deskewing, deconvolution and custom image processing workflows from the terminal</li> <li>Files can be saved as h5 (BigDataViewer/BigStitcher) or tiff files</li> <li>Run in terminal without napari, enabling processing workflows on the HPC</li> </ul> <p>Key Features</p> <p>Apply custom image processing workflows using <code>napari-workflows</code>. </p> <ul> <li>Interactive workflow generation (no coding experience needed)</li> <li>Use custom python functions/modules within workflows</li> <li>How to use Cellpose for cell segmentation</li> </ul> <p>Support will be added for more file formats in the future.</p> <p>Sample lattice lightsheet data download: https://doi.org/10.5281/zenodo.7117784</p> <p>This napari plugin was generated with Cookiecutter using @napari's cookiecutter-napari-plugin template.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are very welcome. Tests can be run with tox, please ensure the coverage at least stays the same before you submit a pull request.</p>"},{"location":"#license","title":"License","text":"<p>Distributed under the terms of the [GPL-3.0 License] license, \"napari_lattice\" is free and open source software</p>"},{"location":"#acknowledgment","title":"Acknowledgment","text":"<p>This project was supported by funding from the Rogers Lab at the Centre for Dynamic Imaging at the Walter and Eliza Hall Institute of Medical Research. This project has been made possible in part by Napari plugin accelerator grant from the Chan Zuckerberg Initiative DAF, an advised fund of the Silicon Valley Community Foundation.</p> <p>Thanks to the developers and maintainers of the amazing open-source plugins such as pyclesperanto, aicsimageio, dask and pycudadecon.  Thanks in particular to the developers of open source projects: LLSpy and lls_dd as they were referred to extensively for developing napari-lattice.  Thanks to the imagesc forum!</p>"},{"location":"#issues","title":"Issues","text":"<p>If you encounter any problems, please file an issue along with a detailed description.</p>"},{"location":"api/","title":"Python Usage","text":""},{"location":"api/#introduction","title":"Introduction","text":"<p>The image processing workflow can also be controlled via Python API.</p> <p>To do so, first define the parameters:</p> <pre><code>from lls_core import LatticeData\n\nparams = LatticeData(\n  input_image=\"/path/to/some/file.tiff\",\n  save_dir=\"/path/to/output\"\n)\n</code></pre> <p>Then save the result to disk: <pre><code>params.save()\n</code></pre></p> <p>Or work with the images in memory: <pre><code>for slice in params.process():\n    pass\n</code></pre></p> <p>Other more advanced options are listed below.</p>"},{"location":"api/#cropping","title":"Cropping","text":"<p>Cropping functionality can be enabled by setting the <code>crop</code> parameter:</p> <pre><code>from lls_core import LatticeData, CropParams \n\nparams = LatticeData(\n  input_image=\"/path/to/some/file.tiff\",\n  save_dir=\"/path/to/output\",\n  crop=CropParams(\n    roi_list=[\"/path/to/roi.zip\"]\n  )\n)\n</code></pre> <p>Other more advanced options are listed below.</p>"},{"location":"api/#type-checking","title":"Type Checking","text":"<p>Because of Pydantic idiosyncrasies, the <code>LatticeData</code> constructor can accept more data types than the type system realises.  For example, <code>input_image=\"/some/path\"</code> like we used above is not considered correct, because ultimately the input image has to become an <code>xarray</code> (aka <code>DataArray</code>). You can solve this in three ways.</p> <p>The first is to use the types precisely as defined. In this case, we might define the parameters \"correctly\" (if verbosely) like this:</p> <pre><code>from lls_core import LatticeData\nfrom aicsimageio import AICSImage\nfrom pathlib import Path\n\nparams = LatticeData(\n  input_image=AICSImage(\"/path/to/some/file.tiff\").xarray_dask_data(),\n  save_dir=Path(\"/path/to/output\")\n)\n</code></pre> <p>The second is to use <code>LatticeData.parse_obj</code>, which takes a dictionary of options and allows incorrect types:</p> <pre><code>params = LatticeData.parse_obj({\n  \"input_image\": \"/path/to/some/file.tiff\",\n  \"save_dir\": \"/path/to/output\"\n})\n</code></pre> <p>Finally, if you're using MyPy, you can install the pydantic plugin, which solves this problem via the <code>init_typed = False</code> option.</p>"},{"location":"api/#api-docs","title":"API Docs","text":""},{"location":"api/#lls_core.LatticeData","title":"lls_core.LatticeData","text":"<p>Parameters for the entire deskewing process, including outputs and optional steps such as deconvolution. This is the recommended entry point for Python users: construct an instance of this class, and then perform the processing using methods.</p> <p>Note that none of this class's methods have any parameters: all parameters are class fields for validation purposes.</p> <p>Parameters:</p> Name Type Description Default <code>input_image</code> <code>DataArray</code> <p>A 3-5D array containing the image data. Can be anything convertible to an Xarray, including a <code>dask.array</code> or <code>numpy.ndarray</code>. Can also be provided as a <code>str</code>, in which case it must indicate the path to an image to load from disk.</p> <code>None</code> <code>skew</code> <code>DeskewDirection</code> <p>Axis along which to deskew the image. Choices: <code>{\"X\", \"Y\"}</code>. These can be provided as <code>str</code>.</p> <code>&lt;DeskewDirection.Y: 2&gt;</code> <code>angle</code> <code>float</code> <p>Angle of deskewing, in degrees, as a float.</p> <code>30.0</code> <code>physical_pixel_sizes</code> <code>DefinedPixelSizes</code> <p>Pixel size of the microscope, in microns. This can alternatively be provided as a <code>tuple[float]</code> of <code>(Z, Y, X)</code></p> <code>None</code> <code>derived</code> <code>DerivedDeskewFields</code> <p>Refer to the <code>DerivedDeskewFields</code> docstring</p> <code>None</code> <code>save_dir</code> <code>DirectoryPath</code> <p>The directory where the output data will be saved. This can be specified as a <code>str</code> or <code>Path</code>.</p> <code>None</code> <code>save_suffix</code> <code>str</code> <p>The filename suffix that will be used for output files. This will be added as a suffix to the input file name if the input image was specified using a file name. If the input image was provided as an in-memory object, the <code>save_name</code> field should instead be specified.</p> <code>'_deskewed'</code> <code>save_name</code> <code>str</code> <p>The filename that will be used for output files. This should not contain a leading directory or file extension. The final output files will have additional elements added to the end of this prefix to indicate the region of interest, channel, timepoint, file extension etc.</p> <code>None</code> <code>save_type</code> <code>SaveFileType</code> <p>The data type to save the result as. This will also be used to determine the file extension of the output files. Choices: <code>{\"h5\", \"tiff\"}</code>. Choices can alternatively be specifed as <code>str</code>, for example <code>'tiff'</code>.</p> <code>&lt;SaveFileType.h5: 'h5'&gt;</code> <code>time_range</code> <code>range</code> <p>The range of times to process. This defaults to all time points in the image array.</p> <code>None</code> <code>channel_range</code> <code>range</code> <p>The range of channels to process. This defaults to all time points in the image array.</p> <code>None</code> <code>deconvolution</code> <code>DeconvolutionParams | None</code> <p>Parameters associated with the deconvolution. If this is None, then deconvolution is disabled</p> <code>None</code> <code>crop</code> <code>CropParams | None</code> <p>Cropping parameters. If this is None, then cropping is disabled</p> <code>None</code> <code>workflow</code> <code>Workflow | None</code> <p>If defined, this is a workflow to add lightsheet processing onto</p> <code>None</code>"},{"location":"api/#lls_core.LatticeData.process","title":"process","text":"<pre><code>process() -&gt; ImageSlices\n</code></pre> <p>Execute the processing and return the result. This will not execute the attached workflow.</p>"},{"location":"api/#lls_core.LatticeData.process_workflow","title":"process_workflow","text":"<pre><code>process_workflow() -&gt; WorkflowSlices\n</code></pre> <p>Runs the workflow on each slice and returns the workflow results</p>"},{"location":"api/#lls_core.LatticeData.save","title":"save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Apply the processing, and saves the results to disk. Results can be found in <code>save_dir</code>.</p>"},{"location":"api/#lls_core.DeconvolutionParams","title":"lls_core.DeconvolutionParams","text":"<p>Parameters for the optional deconvolution step</p> <p>Parameters:</p> Name Type Description Default <code>decon_processing</code> <code>DeconvolutionChoice</code> <p>Hardware to use to perform the deconvolution. Choices: <code>{\"cuda_gpu\", \"opencl_gpu\", \"cpu\"}</code>. Can be provided as <code>str</code>.</p> <code>&lt;DeconvolutionChoice.cpu: 'cpu'&gt;</code> <code>psf</code> <code>List[DataArray]</code> <p>List of Point Spread Functions to use for deconvolution. Each of which should be a 3D array. Each PSF can also be provided as a <code>str</code> path, in which case they will be loaded from disk as images.</p> <code>[]</code> <code>psf_num_iter</code> <code>NonNegativeInt</code> <p>Number of iterations to perform in deconvolution</p> <code>10</code> <code>background</code> <code>float | Literal[str, str]</code> <p>Background value to subtract for deconvolution. Only used when <code>decon_processing</code> is set to <code>GPU</code>. This can either be a literal number, \"auto\" which uses the median of the last slice, or \"second_last\" which uses the median of the last slice.</p> <code>0</code>"},{"location":"api/#lls_core.CropParams","title":"lls_core.CropParams","text":"<p>Parameters for the optional cropping step. Note that cropping is performed in the space of the deskewed shape. This is to support the workflow of performing a preview deskew and using that to calculate the cropping coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>roi_list</code> <code>List[Roi]</code> <p>List of regions of interest, each of which must be an <code>N \u00d7 D</code> array, where N is the number of vertices and D the coordinates of each vertex. This can alternatively be provided as a <code>str</code> or <code>Path</code>, or a list of those, in which case each they are interpreted as paths to ImageJ ROI zip files that are read from disk.</p> <code>[]</code> <code>roi_subset</code> <code>List[int]</code> <p>A subset of all the ROIs to process. Each list item should be an index into the ROI list indicating an ROI to include. This allows you to process only a subset of the regions from a ROI file specified using the <code>roi_list</code> parameter. If <code>None</code>, it is assumed that you want to process all ROIs.</p> <code>None</code> <code>z_range</code> <code>Tuple[NonNegativeInt, NonNegativeInt]</code> <p>The range of Z slices to take as a tuple of the form <code>(first, last)</code>. All Z slices before the first index or after the last index will be cropped out.</p> <code>None</code>"},{"location":"api/#lls_core.models.results.ImageSlices","title":"lls_core.models.results.ImageSlices","text":"<p>A collection of image slices, which is the main output from deskewing. This holds an iterable of output image slices before they are saved to disk, and provides a <code>save_image()</code> method for this purpose.</p> <p>Parameters:</p> Name Type Description Default <code>slices</code> <code>Iterable[ProcessedSlice[Union[Array, ndarray[Any, dtype[+ScalarType]], Array, DataArray]]]</code> <p>Iterable of result slices. For a given slice, you can access the image data through the <code>slice.data</code> property, which is a numpy-like array.</p> <code>None</code> <code>lattice_data</code> <code>ForwardRef('LatticeData')</code> <p>The \"parent\" LatticeData that was used to create this result</p> required"},{"location":"api/#lls_core.models.results.ImageSlices.save_image","title":"save_image","text":"<pre><code>save_image()\n</code></pre> <p>Saves result slices to disk</p>"},{"location":"api/#lls_core.models.results.WorkflowSlices","title":"lls_core.models.results.WorkflowSlices","text":"<p>The counterpart of <code>ImageSlices</code>, but for workflow outputs. This is needed because workflows have vastly different outputs that may include regular Python types rather than only image slices.</p> <p>Parameters:</p> Name Type Description Default <code>slices</code> <code>Iterable[ProcessedSlice[Union[Tuple[Union[Array, ndarray[Any, dtype[+ScalarType]], Array, DataArray, dict, list]], Array, ndarray[Any, dtype[+ScalarType]], Array, DataArray, dict, list]]]</code> <p>Iterable of raw workflow results, the exact nature of which is determined by the author of the workflow. Not typically useful directly, and using he result of <code>.process()</code> is recommended instead.</p> <code>None</code> <code>lattice_data</code> <code>ForwardRef('LatticeData')</code> <p>The \"parent\" LatticeData that was used to create this result</p> required"},{"location":"api/#lls_core.models.results.WorkflowSlices.extract_preview","title":"extract_preview","text":"<pre><code>extract_preview() -&gt; NDArray\n</code></pre> <p>Extracts a single 3D image for previewing purposes</p>"},{"location":"api/#lls_core.models.results.WorkflowSlices.process","title":"process","text":"<pre><code>process() -&gt; Iterable[Tuple[RoiIndex, ProcessedWorkflowOutput]]\n</code></pre> <p>Incrementally processes the workflow outputs, and returns both image paths and data frames of the outputs, for image slices and dict/list outputs respectively</p>"},{"location":"api/#lls_core.models.results.WorkflowSlices.save","title":"save","text":"<pre><code>save() -&gt; Iterable[Path]\n</code></pre> <p>Processes all workflow outputs and saves them to disk. Images are saved in the format specified in the <code>LatticeData</code>, while other data types are saved as a CSV.</p>"},{"location":"api/#lls_core.models.results.ProcessedWorkflowOutput","title":"lls_core.models.results.ProcessedWorkflowOutput  <code>module-attribute</code>","text":"<pre><code>ProcessedWorkflowOutput = Union[Path, DataFrame]\n</code></pre> <p>The result of a workflow. If this is a <code>Path</code>, then it is the path to an image saved to disk. If a <code>DataFrame</code>, then it contains non-image data returned by your workflow.</p>"},{"location":"api/#lls_core.models.deskew.DefinedPixelSizes","title":"lls_core.models.deskew.DefinedPixelSizes","text":"<p>Like PhysicalPixelSizes, but it's a dataclass, and none of its fields are None</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>NonNegativeFloat</code> <p>Size of the X dimension of the microscope pixels, in microns.</p> <code>0.1499219272808386</code> <code>Y</code> <code>NonNegativeFloat</code> <p>Size of the Y dimension of the microscope pixels, in microns.</p> <code>0.1499219272808386</code> <code>Z</code> <code>NonNegativeFloat</code> <p>Size of the Z dimension of the microscope pixels, in microns.</p> <code>0.3</code>"},{"location":"api/#lls_core.models.deskew.DerivedDeskewFields","title":"lls_core.models.deskew.DerivedDeskewFields","text":"<p>Fields that are automatically calculated based on other fields in DeskewParams. Grouping these together into one model makes validation simpler.</p> <p>Parameters:</p> Name Type Description Default <code>deskew_vol_shape</code> <code>Tuple[int, ...]</code> <p>Dimensions of the deskewed output. This is set automatically based on other input parameters, and doesn't need to be provided by the user.</p> <code>None</code> <code>deskew_affine_transform</code> <code>AffineTransform3D</code> <p>Deskewing transformation function. This is set automatically based on other input parameters, and doesn't need to be provided by the user.</p> <code>None</code>"},{"location":"api/#lls_core.models.output.SaveFileType","title":"lls_core.models.output.SaveFileType","text":"<p>Choice of File extension to save</p>"},{"location":"cli/","title":"Command Line Interface","text":""},{"location":"cli/#lls-pipeline","title":"lls-pipeline","text":"<p>Usage:</p> <pre><code>lls-pipeline [OPTIONS] [INPUT_IMAGE]\n</code></pre> <p>Options:</p> <pre><code>  [INPUT_IMAGE]                   Path to the image file to read, in a format\n                                  readable by AICSImageIO, for example .tiff\n                                  or .czi\n  --skew [X|Y]                    Axis along which to deskew the image.\n                                  Choices: `{\"X\", \"Y\"}`. These can be provided\n                                  as `str`.   [default: Y]\n  --angle FLOAT                   Angle of deskewing, in degrees, as a float.\n                                  [default: 30.0]\n  --pixel-sizes &lt;FLOAT FLOAT FLOAT&gt;...\n                                  Pixel size of the microscope, in microns.\n                                  This can alternatively be provided as a\n                                  `tuple[float]` of `(Z, Y, X)` This takes\n                                  three arguments, corresponding to the Z, Y\n                                  and X pixel dimensions respectively\n                                  [default: 0.3, 0.1499219272808386,\n                                  0.1499219272808386]\n  --rois PATH                     A list of paths pointing to regions of\n                                  interest to crop to, in ImageJ format.\n  --roi-indices INTEGER           A subset of all the ROIs to process. Each\n                                  list item should be an index into the ROI\n                                  list indicating an ROI to include. This\n                                  allows you to process only a subset of the\n                                  regions from a ROI file specified using the\n                                  `roi_list` parameter. If `None`, it is\n                                  assumed that you want to process all ROIs.\n  --z-start INTEGER               The index of the first Z slice to use. All\n                                  prior Z slices will be discarded.\n  --z-end INTEGER                 The index of the last Z slice to use. The\n                                  selected index and all subsequent Z slices\n                                  will be discarded. Defaults to the last z\n                                  index of the image.\n  --deconvolution / --disable-deconvolution\n                                  [default: disable-deconvolution]\n  --decon-processing [cuda_gpu|opencl_gpu|cpu]\n                                  Hardware to use to perform the\n                                  deconvolution. Choices: `{\"cuda_gpu\",\n                                  \"opencl_gpu\", \"cpu\"}`. Can be provided as\n                                  `str`.   [default: cpu]\n  --psf PATH                      One or more paths pointing to point spread\n                                  functions to use for deconvolution. Each\n                                  file should in a standard image format\n                                  (.czi, .tiff etc), containing a 3D image\n                                  array. This option can be used multiple\n                                  times to provide multiple PSF files.\n  --psf-num-iter INTEGER          Number of iterations to perform in\n                                  deconvolution   [default: 10]\n  --background TEXT               Background value to subtract for\n                                  deconvolution. Only used when\n                                  `decon_processing` is set to `GPU`. This can\n                                  either be a literal number, \"auto\" which\n                                  uses the median of the last slice, or\n                                  \"second_last\" which uses the median of the\n                                  last slice.   [default: 0]\n  --time-start INTEGER            Index of the first time slice to use\n                                  (inclusive). Defaults to the first time\n                                  index of the image.  [default: 0]\n  --time-end INTEGER              Index of the first time slice to use\n                                  (exclusive). Defaults to the last time index\n                                  of the image.\n  --channel-start INTEGER         Index of the first channel slice to use\n                                  (inclusive). Defaults to the first channel\n                                  index of the image.  [default: 0]\n  --channel-end INTEGER           Index of the first channel slice to use\n                                  (exclusive). Defaults to the last channel\n                                  index of the image.\n  --save-dir PATH                 The directory where the output data will be\n                                  saved. This can be specified as a `str` or\n                                  `Path`.\n  --save-name TEXT                The filename that will be used for output\n                                  files. This should not contain a leading\n                                  directory or file extension. The final\n                                  output files will have additional elements\n                                  added to the end of this prefix to indicate\n                                  the region of interest, channel, timepoint,\n                                  file extension etc.\n  --save-type [h5|tiff]           The data type to save the result as. This\n                                  will also be used to determine the file\n                                  extension of the output files. Choices:\n                                  `{\"h5\", \"tiff\"}`. Choices can alternatively\n                                  be specifed as `str`, for example `'tiff'`.\n                                  [default: h5]\n  --workflow PATH                 Path to a Napari Workflow file, in YAML\n                                  format. If provided, the configured\n                                  desekewing processing will be added to the\n                                  chosen workflow.\n  --json-config PATH              Path to a JSON file from which parameters\n                                  will be read.\n  --yaml-config PATH              Path to a YAML file from which parameters\n                                  will be read.\n  --show-schema / --no-show-schema\n                                  If provided, image processing will not be\n                                  performed, and instead a JSON document\n                                  outlining the JSON/YAML options will be\n                                  printed to stdout. This can be used to\n                                  assist with writing a config file for use\n                                  with the --json-config and --yaml-config\n                                  options.  [default: no-show-schema]\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"development/","title":"Development","text":""},{"location":"development/#structure","title":"Structure","text":"<p>The repo is divided into two python packages: <code>core</code> (the core processing and CLI) and <code>plugin</code> (the Napari GUI). Within each directory is:</p> <ul> <li>The python package</li> <li><code>pyproject.toml</code>, the package metadata such as the dependencies, and </li> <li><code>tests/</code>, which contains the tests</li> </ul>"},{"location":"development/#installation","title":"Installation","text":"<p>For local development, first clone the repo and then run the following from the repository root: <pre><code>pip install -e core -e plugin\n</code></pre></p>"},{"location":"development/#technologies","title":"Technologies","text":""},{"location":"development/#pydantic","title":"Pydantic","text":"<p>Used for defining the parameter sets, performing parameter validation and conversion. These models live in <code>core/lls_core/models</code>. Note, <code>lls_core</code> uses Pydantic 1.X, which has a different API to Pydantic 2.X. You can find relevant documentation here: https://docs.pydantic.dev/1.10/</p>"},{"location":"development/#xarray","title":"Xarray","text":"<p>Used for all image representations, where they are treated as multidimensional arrays with dimensional labels (X, Y, Z etc). Refer to: https://xarray.pydata.org/.</p>"},{"location":"development/#typer","title":"Typer","text":"<p>The CLI is defined using Typer: https://typer.tiangolo.com/.</p>"},{"location":"development/#magicgui-and-magicclass","title":"magicgui and magicclass","text":"<p>These packages are used to define the GUI, which you can find in <code>plugin/napari_lattice</code>. <code>magicclass</code> builds on <code>magicgui</code> by providing the <code>@magicclass</code> decorator which turns a Python class into a GUI.</p>"},{"location":"development/#dev-workflows","title":"Dev Workflows","text":""},{"location":"development/#adding-a-new-parameter","title":"Adding a new parameter","text":"<p>Whenever a new parameter is added, the following components need to be updated:</p> <ul> <li>Add the parameter to the Pydantic models</li> <li>Add the parameter to the CLI (<code>core/lls_core/cmds/__main__.py</code>), and define mapping between CLI and Pydantic using the <code>CLI_PARAM_MAP</code></li> <li>Add the field to the GUI in <code>plugin/napari_lattice/fields.py</code></li> <li>Define the new processing logic in <code>core/lls_core/models/lattice_data.py</code></li> </ul> <p>An example of this can be found in this commit: https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/pull/47/commits/16b28fec307f19e73b8d55e677621082037b2710.</p>"},{"location":"development/#adding-a-new-image-reader","title":"Adding a new image reader","text":"<p>Currently there aren't image reader classes. Instead, we currently have a pydantic validator that converts the image from a path to an array, or from an array into an xarray. A new format could be implemented in this validator: https://github.com/BioimageAnalysisCoreWEHI/napari_lattice/blob/b33cc4ca5fe0fb89d730cefdbe3169f984f1fe89/core/lls_core/models/deskew.py#L176-L202</p>"},{"location":"development/#adding-a-new-image-writer","title":"Adding a new image writer","text":"<ol> <li> <p>Create a new writer which inherits from the <code>lls_core.writers.Writer</code> class, and implements its <code>write_slice</code> method:</p> </li> <li> <p>Add a new option to the <code>SaveFileType</code> enum</p> </li> <li> <p>Then, return the correct writer class based on the enum value.</p> </li> </ol>"},{"location":"development/#lls_core.writers.Writer.write_slice","title":"lls_core.writers.Writer.write_slice  <code>abstractmethod</code>","text":"<pre><code>write_slice(slice: ProcessedSlice[ArrayLike])\n</code></pre> <p>Writes a 3D image slice</p>"},{"location":"development/#testing","title":"Testing","text":"<p>The tests are run using pytest. To install the testing dependencies, use <code>pip install -e 'core[testing]' -e 'plugin[testing]'</code> Since there are two separate packages, you will have to specify the location of each test directory. To run all the tests, use <code>pytest core/tests/ plugin/tests</code> from the root directory.</p>"},{"location":"development/#documentation","title":"Documentation","text":"<p>Docs are built with mkdocs.</p> <p>To modify the docs, you need the docs dependencies, so clone the repo and then:</p> <pre><code>pip install -e 'core[docs]'\n</code></pre> <p>The key files are:</p> <pre><code>* `mkdocs.yml`, which is the main config file for mkdocs, and\n* `docs/` which is a directory containing markdown files. Each new file that gets added there will create a new page in the website.\n</code></pre> <p>Some useful <code>mkdocs</code> commands:</p> <ul> <li><code>mkdocs serve</code> runs the development server which hosts the docs on a local web server. Any changes to your markdown files will be reflected in this server, although you sometimes have to restart the server if you make a change to configuration</li> <li><code>mkdocs gh-deploy</code> builds the docs, and pushes them to GitHub Pages. This updates the docs at https://bioimageanalysiscorewehi.github.io/napari_lattice/.</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>It is advisable but not required that you install Napari Lattice packages inside a conda environment. This is because conda makes it much easier to install system complex dependencies.</p> <p>To install the core package, which includes the Python library and command line interface:</p> <pre><code>pip install lls-core\n</code></pre> <p>To install the Napari plugin:</p> <pre><code>pip install napari-lattice\n</code></pre>"},{"location":"installation/#cuda-deconvolution","title":"CUDA Deconvolution","text":"<p>If you have access to a CUDA-compatible GPU, you can enable deconvolution using <code>pycudadecon</code>.</p> <p>If you're using conda (or micromamba etc), you can run the following:</p> <pre><code>conda install -c conda-forge pycudadecon\n</code></pre> <p>Otherwise, you will have to manually ensure the systems dependencies are installed, and then:</p> <pre><code>pip install lls-core[deconvolution]\n</code></pre>"},{"location":"installation/#development-versions","title":"Development Versions","text":"<p>To install the development version of <code>lls-core</code>:</p> <pre><code>pip install git+https://github.com/BioimageAnalysisCoreWEHI/napari_lattice.git#subdirectory=core\n</code></pre> <p>For <code>napari-lattice</code>:</p> <pre><code>pip install git+https://github.com/BioimageAnalysisCoreWEHI/napari_lattice.git#subdirectory=plugin\n</code></pre>"},{"location":"workflow/","title":"Workflows","text":"<p><code>lls_core</code> supports integration with <code>napari-workflows</code>. The advantage of this is that you can design a multi-step automated workflow that uses <code>lls_core</code> as the pre-processing step.</p>"},{"location":"workflow/#building-a-workflow","title":"Building a Workflow","text":"<p>You can design your workflow via GUI using `napari-assistant, or in the YAML format.</p> <p>When building your workflow with Napari Assistant, you are actually building a template that will be applied to future images. For this reason, you need to rename your input layer to <code>deskewed_image</code>, since this is the exact value that the <code>lls_core</code> step produces.</p> <p>If you want to use YAML, you also have to make sure that the first workflow step to run takes <code>deskewed_image</code> as an input. For example:</p> <pre><code>!!python/object:napari_workflows._workflow.Workflow\n_tasks:\n  median: !!python/tuple\n  - !!python/name:pyclesperanto_prototype.median_sphere ''\n  - deskewed_image\n  - null\n  - 2\n  - 2\n  - 2\n</code></pre> <p>Workflows are run once for each 3D slice of the image. In other words, the workflow is run separately for each timepoint, for each channel, for each region of interest (if cropping is enabled). This means that you should design your workflow expecting that <code>deskewed_image</code> is an exactly 3D array.</p> <p>If you want to define your own custom functions, you can do so in a <code>.py</code> file in the same directory as the workflow <code>.yml</code> file.  These will be imported before the workflow is executed.</p>"},{"location":"workflow/#running-a-workflow","title":"Running a Workflow","text":"<p>The <code>--workflow</code> command-line flag, the <code>LatticeData(workflow=)</code> Python parameter, and the Workflow tab of the plugin can be used to specify the path to a workflow <code>.yml</code> file .</p> <p>If you're using the Python interface, you need to use <code>LatticeData.process_workflow()</code> rather than <code>.process()</code>. </p>"},{"location":"workflow/#outputs","title":"Outputs","text":"<p><code>lls_core</code> supports workflows that have exactly one \"leaf\" task. This is defined as a task that is not used by any other tasks. In other works, it's the final task of the workflow.</p> <p>If you want multiple return values, this task can return a tuple of values. Each of these values must be:</p> <ul> <li>An array, in which case it is treated as an image slice</li> <li>A <code>dict</code>, in which case it is treated as a single row of a data frame whose columns are the keys of the <code>dict</code></li> <li>A <code>list</code>, in which case it is treated as a single row of a data frame whose columns are unnamed</li> </ul> <p>Then, each slice is combined at the end. Image slices are stacked together into their original dimensionality, and data frame rows are stacked into a data frame with one row per channel, timepoint and ROI.</p>"}]}